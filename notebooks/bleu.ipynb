{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sentence_bleu in module nltk.translate.bleu_score:\n",
      "\n",
      "sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None, auto_reweigh=False)\n",
      "    Calculate BLEU score (Bilingual Evaluation Understudy) from\n",
      "    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\n",
      "    \"BLEU: a method for automatic evaluation of machine translation.\"\n",
      "    In Proceedings of ACL. https://www.aclweb.org/anthology/P02-1040.pdf\n",
      "    \n",
      "    >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
      "    ...               'ensures', 'that', 'the', 'military', 'always',\n",
      "    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
      "    \n",
      "    >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n",
      "    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n",
      "    ...               'that', 'party', 'direct']\n",
      "    \n",
      "    >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
      "    ...               'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
      "    ...               'heed', 'Party', 'commands']\n",
      "    \n",
      "    >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
      "    ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
      "    ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
      "    ...               'Party']\n",
      "    \n",
      "    >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
      "    ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
      "    ...               'of', 'the', 'party']\n",
      "    \n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS\n",
      "    0.5045...\n",
      "    \n",
      "    If there is no ngrams overlap for any order of n-grams, BLEU returns the\n",
      "    value 0. This is because the precision for the order of n-grams without\n",
      "    overlap is 0, and the geometric mean in the final BLEU score computation\n",
      "    multiplies the 0 with the precision of other n-grams. This results in 0\n",
      "    (independently of the precision of the other n-gram orders). The following\n",
      "    example has zero 3-gram and 4-gram overlaps:\n",
      "    \n",
      "    >>> round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4) # doctest: +ELLIPSIS\n",
      "    0.0\n",
      "    \n",
      "    To avoid this harsh behaviour when no ngram overlaps are found a smoothing\n",
      "    function can be used.\n",
      "    \n",
      "    >>> chencherry = SmoothingFunction()\n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis2,\n",
      "    ...     smoothing_function=chencherry.method1) # doctest: +ELLIPSIS\n",
      "    0.0370...\n",
      "    \n",
      "    The default BLEU calculates a score for up to 4-grams using uniform\n",
      "    weights (this is called BLEU-4). To evaluate your translations with\n",
      "    higher/lower order ngrams, use customized weights. E.g. when accounting\n",
      "    for up to 5-grams with uniform weights (this is called BLEU-5) use:\n",
      "    \n",
      "    >>> weights = (1./5., 1./5., 1./5., 1./5., 1./5.)\n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n",
      "    0.3920...\n",
      "    \n",
      "    Multiple BLEU scores can be computed at once, by supplying a list of weights.\n",
      "    E.g. for computing BLEU-2, BLEU-3 *and* BLEU-4 in one computation, use:\n",
      "    >>> weights = [\n",
      "    ...     (1./2., 1./2.),\n",
      "    ...     (1./3., 1./3., 1./3.),\n",
      "    ...     (1./4., 1./4., 1./4., 1./4.)\n",
      "    ... ]\n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n",
      "    [0.7453..., 0.6240..., 0.5045...]\n",
      "    \n",
      "    :param references: reference sentences\n",
      "    :type references: list(list(str))\n",
      "    :param hypothesis: a hypothesis sentence\n",
      "    :type hypothesis: list(str)\n",
      "    :param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\n",
      "    :type weights: tuple(float) / list(tuple(float))\n",
      "    :param smoothing_function:\n",
      "    :type smoothing_function: SmoothingFunction\n",
      "    :param auto_reweigh: Option to re-normalize the weights uniformly.\n",
      "    :type auto_reweigh: bool\n",
      "    :return: The sentence-level BLEU score. Returns a list if multiple weights were supplied.\n",
      "    :rtype: float / list(float)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sentence_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/bleu-score-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
